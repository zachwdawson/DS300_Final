{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> DS 3000 - Fall 2020</h2> </center>\n",
    "<center> <h3> DS Report </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Identifying Splice Junctions Using Supervised Machine Learning</h3> </center>\n",
    "<center><h4>Charles Crain, Zachary Dawson, Giona Kleinberg</h4></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executive Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splice junction identification is an integral component of identifying disease-causing genetic variations and other important biomedical efforts. The goal of this project was to apply an optimal machine learning algorithm capable of classifying the type of splice-junction present within a genetic sequence. The raw data used contained over 3000 genetic sequences containing information on the nucleotide base at 60 sequential positions centered around each labeled junction.  The data was cleaned, preprocessed, visualized, and transformed using one-hot encoding. GC content was extracted as an additional feature and feature selection was then performed. Out of the supervised classification algorithms used, the Binomial Naive Bayes algorithm performed the best with a mean accuracy of 96.32%. A significant difference was found in GC content between the types of splice junctions using a Kruskal Wallis test (p<0.0001) and nucleotide bases at positions closer to the splice junction were found more important to the created model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. <a href='#1'>INTRODUCTION</a>\n",
    "2. <a href='#2'>METHOD</a>\n",
    "3. <a href='#3'>RESULTS</a>\n",
    "4. <a href='#4'>DISCUSSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is the problem?**\n",
    "\n",
    "\n",
    "In all living organisms, the DNA genetic code is first transcribed into messenger RNA (mRNA) transcripts which go on to direct protein synthesis. In eukaryotic organisms, the initial pre-mRNA transcripts consist of exons, which code for the protein, and introns, which are non-coding sequences. The exons must be spliced together to form the final mRNA transcript in order to direct protein synthesis. This splicing process involves stitching together the exons while removing the introns. \n",
    "\n",
    " \n",
    "Due to advances in genomics technology, the full genome sequences of many organisms are known. However, one of the challenges to accurately predicting proteins in eukaryotes is determining which sequences in genes are exons and introns; the solution to this challenge involves locating the junctions between exons and introns that the cell recognizes during splicing. There are many sequence and context-dependent factors that determine which parts of sequences are splice sites (De Conti, Baralle, & Buratti, 2013). **The aim of our project is to find an optimal machine learning model for predicting splice sites from DNA sequence.**\n",
    "\n",
    "\n",
    "\n",
    "### **Why is it important?**\n",
    "\n",
    "\n",
    "As mentioned above, splice site recognition is important to predicting the coding sequences of genes in humans and other eukaryotic organisms. Additionally, detecting splice sites is also crucial for predicting isoforms of genes, which are similar proteins formed from splicing together exons in different ways. It is estimated that 9-11% of rare genetic disorders involve cryptic splice sites, or mutations that introduce a splice site where there should not be one (Jaganathan et al., 2019). Thus, determination of splice site is important to identifying and characterizing disease-causing genetic variation. For all of these reasons, splice site prediction is an important biomedical goal which may benefit from the application of machine learning. We are very interested to learn if there is a strong enough relation between the nucleotide bases at each position in a genetic sequence and the type of splice junction present in that sequence.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### **Previous Works**\n",
    "\n",
    "\n",
    "This problem has been previously studied in a machine learning context.  Meher, et al. (2019) tested the performance of five classification algorithms in combination with 8 nucleotide-encoding schemes. Their research found that, both in terms of accuracy and computational speed, the best performing classifier was a support vector machine. They found that SVM performed at >90% accuracy across multiple encoding schemes. The best performing encoding scheme was a nucleotide-specific encoder calleddi-nucleotide frequency difference between real & false splice sites (FDTF). Jaganathan, et al. (2019) developed a deep residual neural network to predict splice sites with 95% accuracy based on 10,000 nucleotides of adjacent sequence. \n",
    "\n",
    " \n",
    " \n",
    "### **Questions and Hypotheses**\n",
    "\n",
    "\n",
    "There is a large magnitude of information that can be obtained through the use of machine learning. For this particular report, we set out to answer the following questions:\n",
    "\n",
    "\n",
    "**Question 1:** What type of machine learning model will best be able to predict the type of boundary in a genetic sequence?\n",
    "\n",
    "**Hypothesis:** The Support Vector Machine, Bernoulli Naive Bayes, and Multinomial Naive Bayes will perform the best out of the basic set of classification algorithms.\n",
    "\n",
    "**Justification:** The Support Vector Machine algorithm is predicted to do well due to the high dimensions of the dataset. The Multinomial Naive Bayes algorithm is designed to handle data with discrete features which resonates well with our datasets' features which are primarily discrete nucleotide bases at given positions in a genetic sequence. The Bernoulli Naive Bayes is also predicted to perform well as we plan to one-hot encode our nucleotide base features into discrete, binary features which the Bernoulli Naive Bayes model is adept at handling.\n",
    "\n",
    "\n",
    "**Question 2:** Is there a significant difference in GC content between the different types of splice junctions?\n",
    "\n",
    "**Hypothesis:** There is a significant difference in GC content between the different types of splice junctions.\n",
    "\n",
    "**Justification:** GC content provides valuable insight into the genetic contents of a given sequence so it makes sense that the type of junction within a genetic sequence will affect the GC content. Genes typically have a higher GC content than non-encoding regions of the genome so since splice junctions are contained within genes, their GC content will likely differ especially from non-splice junctions.\n",
    "\n",
    "\n",
    "**Question 3:** Where are the most important locations within a genetic sequence in predicting the type of splice junction present within that sequence?\n",
    "\n",
    "**Hypothesis:** The most important positions will be centered around the the location of the junction.\n",
    "\n",
    "**Justification:** The nucleotide bases in positions closer to the center of the junction will likely have a greater impact in determining the type of splice junction and will therefore likely be very influential as features of a machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "### **References**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "De Conti, L., Baralle, M., & Buratti, E. (2013). Exon and intron definition in pre-mRNA splicing. Wiley Interdiscip Rev RNA, 4(1), 49-60. doi:10.1002/wrna.1140 \n",
    "\n",
    "Jaganathan, K., Kyriazopoulou Panagiotopoulou, S., McRae, J. F., Darbandi, S. F., Knowles, D., Li, Y. I., . . . Farh, K. K.-H. (2019). Predicting Splicing from Primary Sequence with Deep Learning. Cell, 176(3), 535-548.e524. doi:https://doi.org/10.1016/j.cell.2018.12.015 \n",
    "\n",
    "Meher, P. K., Sahu, T. K., Gahoi, S., Satpathy, S., & Rao, A. R. (2019). Evaluating the performance of sequence encoding schemes and machine learning methods for splice sites recognition. Gene, 705, 113-126. doi:https://doi.org/10.1016/j.gene.2019.04.047 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data was obtained from openML which is an online dataset repository containing datasets that can be analyzed via machine learning. The link to the page with the original dataset is https://www.openml.org/d/46. The sequences of the dataset were originally found in GenBank 64.1 and were donated by G. Towell, M. Noordewier, and J. Shavlik to openML to be analyzed using machine learning. The data was read into JupyterLab from GitHub.\n",
    "    \n",
    "Our dataset represents a set of sequences taken from primates originally from GenBank 64.1. There are 3,190 rows each representing a genetic sequence that can be classified by the target variable. Each of the 60 features of the dataset represent the nucleotide base at a position in the sequence. The first 30 positions are before the predicted splice junction and the next 30 positions are after the predicted splice junction. The nucleotide base is either A, C, T, G, and the could also be classified as D, N, S, R. If the positional value is D, the nucleotide base is either A, G, or T. If the positional value is N, the base could be A, C, T, or G. If the positional value is S, the nucleotide base is either C or G. If the positional value is R, the nucleotide base is either A or G. The target variable is the classification of the sequence into one that has an Exon/Intron Boundary (EI), an Intron/Exon Boundary (IE) or a non-splice junction (N). \n",
    "    \n",
    "The first code block loads in the raw data from GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instance_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATRINS-DONOR-521</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATRINS-DONOR-905</th>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BABAPOE-DONOR-30</th>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BABAPOE-DONOR-867</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BABAPOE-DONOR-2817</th>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   attribute_1 attribute_2 attribute_3 attribute_4  \\\n",
       "Instance_name                                                        \n",
       "ATRINS-DONOR-521             C           C           A           G   \n",
       "ATRINS-DONOR-905             A           G           A           C   \n",
       "BABAPOE-DONOR-30             G           A           G           G   \n",
       "BABAPOE-DONOR-867            G           G           G           C   \n",
       "BABAPOE-DONOR-2817           G           C           T           C   \n",
       "\n",
       "                   attribute_5 attribute_6 attribute_7 attribute_8  \\\n",
       "Instance_name                                                        \n",
       "ATRINS-DONOR-521             C           T           G           C   \n",
       "ATRINS-DONOR-905             C           C           G           C   \n",
       "BABAPOE-DONOR-30             T           G           A           A   \n",
       "BABAPOE-DONOR-867            T           G           C           G   \n",
       "BABAPOE-DONOR-2817           A           G           C           C   \n",
       "\n",
       "                   attribute_9 attribute_10  ... attribute_52 attribute_53  \\\n",
       "Instance_name                                ...                             \n",
       "ATRINS-DONOR-521             A            T  ...            G            C   \n",
       "ATRINS-DONOR-905             C            G  ...            T            G   \n",
       "BABAPOE-DONOR-30             G            G  ...            A            C   \n",
       "BABAPOE-DONOR-867            T            T  ...            G            T   \n",
       "BABAPOE-DONOR-2817           C            C  ...            C            T   \n",
       "\n",
       "                   attribute_54 attribute_55 attribute_56 attribute_57  \\\n",
       "Instance_name                                                            \n",
       "ATRINS-DONOR-521              C            A            G            T   \n",
       "ATRINS-DONOR-905              C            C            C            C   \n",
       "BABAPOE-DONOR-30              G            G            G            G   \n",
       "BABAPOE-DONOR-867             T            T            T            C   \n",
       "BABAPOE-DONOR-2817            T            G            A            C   \n",
       "\n",
       "                   attribute_58 attribute_59 attribute_60 Class  \n",
       "Instance_name                                                    \n",
       "ATRINS-DONOR-521              C            T            G    EI  \n",
       "ATRINS-DONOR-905              C            G            C    EI  \n",
       "BABAPOE-DONOR-30              A            T            G    EI  \n",
       "BABAPOE-DONOR-867             C            C            C    EI  \n",
       "BABAPOE-DONOR-2817            C            C            T    EI  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # import pandas\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/dawsonz17/DS300project/master/data.csv' # url to github raw csv\n",
    "df = pd.read_csv(url, index_col=0) # reads online csv from github into a pandas dataframe\n",
    "df.head(5) # displays head of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   For the first hypothesis, the independent variable is the the type of machine learning model, and the dependent variable is the model's performance (accuracy score). For the second hypothesis, the independent variable is the type of slice junction (our target variable) and the dependent variable is the GC content of the genetic sequences. For the third hypothesis, the independent variable is the position features in the genetic sequence, and the dependent variable is the importance of the feature to creating a supervised classification model (measured discretely as important or not using univariate feature selection).\n",
    "\n",
    "\n",
    "   **The goal of the predictive model is to predict the type of splice-junction (target variable) from the nucleotide bases at 60 different positions across a genetic sequence as well as the GC content of the sequence (feature variables).** These features were chosen as they provide information regarding the building blocks of a splice-junction (nucleotide bases) as well as patterns in their order and quantity. Due to the close association of the feature variables and target variable, these features should provide a relaible foundation for predicting the target variable.\n",
    "\n",
    "\n",
    "   This problem is a supervised machine leaning problem as a model is being trained using labeled data. By providing a complementary training set and validation set to the chosen algorithms, machine learning will allow the creation of a model that can label unlabeled data in a testing set or additional data obtained in the future. **The sub-category of supervised machine learning that our task falls into is multiclass classification** as our target variable is discrete and there are three possible classes for each item to be labeled as.\n",
    "\n",
    "\n",
    "   The machine learning algorithms used are k-Nearest Neighbors, Support Vector Machine, Guassian Naive Bayes, Decision Tree, Binomial Naive Bayes, and Multinomial Naive Bayes. The support vector machine algorithm was chosen due to its strength as an algorithm to predict high dimensional data. The Multinomial Naive Bayes algorithm was chosen since it is optimized for discrete datasets such as ours. The Binomial Naive Bayes algorithm was chosen for a similar reason as the algorithm performs well on discrete data and more specifically binary data. While the majority of our features are not binary (A, C, T, or G), they will likely function similar to binary features once one-hot-encoding is used. Due to this reason, the Binomial Naive Bayes was included. The other used algorithms, including the K-Nearest Neighbors, Gaussian Naive Bayes, and Decision Tree algorithms were not as suited for our dataset however were included as well due to the low computational cost of doing so. Finding the algorithm with the best performance was prioritized over the neglible computational cost of testing these additional algorithms even though the chance of their success was relatively lower than the other algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Variable Isolation\n",
    "This code block isolates the features from the imported dataset. This is necessary in order to manage the features throughout the rest of the process. The output for this cell is the head of the feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_51</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  attribute_1 attribute_2 attribute_3 attribute_4 attribute_5 attribute_6  \\\n",
       "0           C           C           A           G           C           T   \n",
       "1           A           G           A           C           C           C   \n",
       "2           G           A           G           G           T           G   \n",
       "3           G           G           G           C           T           G   \n",
       "4           G           C           T           C           A           G   \n",
       "\n",
       "  attribute_7 attribute_8 attribute_9 attribute_10  ... attribute_51  \\\n",
       "0           G           C           A            T  ...            A   \n",
       "1           G           C           C            G  ...            G   \n",
       "2           A           A           G            G  ...            C   \n",
       "3           C           G           T            T  ...            G   \n",
       "4           C           C           C            C  ...            C   \n",
       "\n",
       "  attribute_52 attribute_53 attribute_54 attribute_55 attribute_56  \\\n",
       "0            G            C            C            A            G   \n",
       "1            T            G            C            C            C   \n",
       "2            A            C            G            G            G   \n",
       "3            G            T            T            T            T   \n",
       "4            C            T            T            G            A   \n",
       "\n",
       "  attribute_57 attribute_58 attribute_59 attribute_60  \n",
       "0            T            C            T            G  \n",
       "1            C            C            G            C  \n",
       "2            G            A            T            G  \n",
       "3            C            C            C            C  \n",
       "4            C            C            C            T  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop('Class', axis=1).reset_index(drop=True) # isolate features\n",
    "features.head(5) # show head of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable Isolation\n",
    "\n",
    "The next code block isolates the target variable from the imported dataset. Similar to the feature isolation, it is necessarry to wrangle the target variable in order to have access to it in later steps. The output for this cell is the head of the target dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    EI\n",
       "1    EI\n",
       "2    EI\n",
       "3    EI\n",
       "4    EI\n",
       "Name: Class, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['Class'].reset_index(drop=True) # isolate target\n",
    "target.head(5) # shows head of dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Features - One-Hot Encoding of Feature Variables\n",
    "\n",
    "This code block encodes all 60 features of the dataset using One-Hot-Encoding. This is necessary because the first 60 features of the dataset are discrete categorical values that cannot be used by machine learning models and need to be transformed into quantitative variables. The output for this cell is the head of the updated features dataframe resulting from One-Hot-Encoding. It can be seen that this step greatly increases the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_A</th>\n",
       "      <th>x0_C</th>\n",
       "      <th>x0_D</th>\n",
       "      <th>x0_G</th>\n",
       "      <th>x0_T</th>\n",
       "      <th>x1_A</th>\n",
       "      <th>x1_C</th>\n",
       "      <th>x1_D</th>\n",
       "      <th>x1_G</th>\n",
       "      <th>x1_T</th>\n",
       "      <th>...</th>\n",
       "      <th>x58_A</th>\n",
       "      <th>x58_C</th>\n",
       "      <th>x58_G</th>\n",
       "      <th>x58_N</th>\n",
       "      <th>x58_T</th>\n",
       "      <th>x59_A</th>\n",
       "      <th>x59_C</th>\n",
       "      <th>x59_G</th>\n",
       "      <th>x59_N</th>\n",
       "      <th>x59_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_A  x0_C  x0_D  x0_G  x0_T  x1_A  x1_C  x1_D  x1_G  x1_T  ...  x58_A  \\\n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0   0.0  ...    0.0   \n",
       "4   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   x58_C  x58_G  x58_N  x58_T  x59_A  x59_C  x59_G  x59_N  x59_T  \n",
       "0    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0  \n",
       "1    0.0    1.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0  \n",
       "3    1.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "4    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "\n",
       "[5 rows x 287 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder # import one hot encoding\n",
    "\n",
    "encoder = OneHotEncoder(sparse = False) # create encoder\n",
    "encoded_f = encoder.fit_transform(features) # get encoded features\n",
    "features_encoded = pd.DataFrame(encoded_f, columns= encoder.get_feature_names()) # puts encoded features into a dataframe\n",
    "features_encoded.head(5) # shows head of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Cleaning\n",
    "\n",
    "The next code block replaces the target variable's values with numbers instead of strings/characters to clean up the data further. Cleaning the data by replacing the current values with numbers allows the machine learning algorithms to better use the target variable. The output for this cell is the head of the updated target dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetvars = {'EI': 0, 'IE': 1, 'N': 2} # create dictionary for target variable\n",
    "target = target.replace(targetvars) # replace target variables with numerical equivalents\n",
    "target.head(5) # show head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction of GC Content - Preprocess (Normalize Feature)\n",
    "\n",
    "The following code block allows us to perform feature extraction on our data to find the GC content of each row (sequence). GC content is a useful metric to quantify the nucleotide base content of a genetic sequence and is predicted to also be helpful in classifying the type of splice-junction present in a genetic sequence. GC content is simply the ratio of G and C nucleotide bases to other bases in a sequence and provides information on the contents of the sequence. Other, similar metrics such as AT content are not calculated (and rarely are in similar problems) as they provide no additional information since knowing the G and C content of a gentic sequence provides the AT content (AT content = 1-GC content) The output for this cell shows the head of the updated features dataframe with GC content. GC Content is normalized to an 0-100% scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_A</th>\n",
       "      <th>x0_C</th>\n",
       "      <th>x0_D</th>\n",
       "      <th>x0_G</th>\n",
       "      <th>x0_T</th>\n",
       "      <th>x1_A</th>\n",
       "      <th>x1_C</th>\n",
       "      <th>x1_D</th>\n",
       "      <th>x1_G</th>\n",
       "      <th>x1_T</th>\n",
       "      <th>...</th>\n",
       "      <th>x58_C</th>\n",
       "      <th>x58_G</th>\n",
       "      <th>x58_N</th>\n",
       "      <th>x58_T</th>\n",
       "      <th>x59_A</th>\n",
       "      <th>x59_C</th>\n",
       "      <th>x59_G</th>\n",
       "      <th>x59_N</th>\n",
       "      <th>x59_T</th>\n",
       "      <th>gccontent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_A  x0_C  x0_D  x0_G  x0_T  x1_A  x1_C  x1_D  x1_G  x1_T  ...  x58_C  \\\n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0   0.0  ...    1.0   \n",
       "4   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0  ...    1.0   \n",
       "\n",
       "   x58_G  x58_N  x58_T  x59_A  x59_C  x59_G  x59_N  x59_T  gccontent  \n",
       "0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   0.633333  \n",
       "1    1.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   0.800000  \n",
       "2    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   0.683333  \n",
       "3    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   0.650000  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   0.666667  \n",
       "\n",
       "[5 rows x 288 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = features.apply(pd.Series.value_counts, axis=1)[['C', 'G', 'S']] # get count of C, and G, and S (since S is C or G)\n",
    "# D, N, R are not included since we cannot be sure it's G or C.\n",
    "\n",
    "counts = counts.fillna(0) # gets rid of Null values since if there is no G or C, the gc content is 0\n",
    "counts['gccontent'] = (counts['C'] + counts['G']) / 60  # get gccontent and normalize\n",
    "featurese = pd.concat((features_encoded, counts['gccontent']), axis=1) # add gccontent to features\n",
    "featurese.head(5) # show head of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "\n",
    "The next code block allows feature selection of our feature variables. Univariate feature selection is used to select the most significant features in order to create a model with the best results. The value for k of the number of features to select was found by hypertuning the parameter. This was necessary to ensure significant features were not eliminated while insignificant features were eliminated and unable to adversely affect the models' performance. The list of selected features used to create our model is output by this cell. A pattern can already be noticed in the features that are the most significant. The chosen features represent the nucleotide bases very close to the center of each genetic sequence. GC content was also selected in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giona\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [78] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "C:\\Users\\giona\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['x8_G', 'x9_G', 'x15_A', 'x15_T', 'x16_A', 'x16_T', 'x17_A', 'x17_G',\n",
       "       'x17_T', 'x18_A', 'x18_G', 'x18_T', 'x19_A', 'x19_T', 'x20_A', 'x20_G',\n",
       "       'x20_T', 'x21_A', 'x21_G', 'x21_T', 'x22_A', 'x22_C', 'x22_G', 'x22_T',\n",
       "       'x23_A', 'x23_C', 'x23_G', 'x24_A', 'x24_C', 'x24_G', 'x24_T', 'x25_A',\n",
       "       'x25_G', 'x25_T', 'x27_A', 'x27_C', 'x27_G', 'x28_A', 'x28_C', 'x28_G',\n",
       "       'x28_T', 'x29_A', 'x29_C', 'x29_G', 'x29_T', 'x30_A', 'x30_C', 'x30_G',\n",
       "       'x30_T', 'x31_A', 'x31_C', 'x31_G', 'x31_T', 'x32_A', 'x32_C', 'x32_G',\n",
       "       'x32_T', 'x33_A', 'x33_C', 'x33_T', 'x34_A', 'x34_C', 'x34_G', 'x34_T',\n",
       "       'x35_T', 'gccontent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif # import feature selection for classification\n",
    "from sklearn.feature_selection import SelectKBest # import feature selection\n",
    "from sklearn.model_selection import train_test_split # import train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(featurese, target, random_state=3000) # split dataset\n",
    "select = SelectKBest(score_func=f_classif, k=66) # create selector\n",
    "select.fit(X_train, y_train) # fit selector to training set\n",
    "features_selected = select.transform(featurese) # transform features dataframe to features kept\n",
    "X_train_selected = select.transform(X_train) # transform training set\n",
    "X_test_selected = select.transform(X_test) # transform testing set\n",
    "selected_feature_list = featurese.columns[select.get_support()] # variable containing features used\n",
    "selected_feature_list # outputs features used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1 - Stacked Bar Chart of Nucleotide Frequencies\n",
    "\n",
    "For our first plot, we created a stacked bar chart to visualize the nucleotide frequencies at each position in the sequences. ‘0’ is not a real position, but instead demarcates the center of the sequence. Positions -30 to -1 are the 30 nucleotides anterior to the splice site, whereas positions 1 to 30 are the 30 nucleotides posterior to the splice site.\n",
    "\n",
    "Most positions have relatively uniform distributions of the four nucleotide bases, but certain bases are overrepresented at the positions closest to the splice site. In particular, the nucleotide G is particularly prevalent at the -1 and +1 positions. This points to unique nucleotide patterns close to the hypothesized splice site. These patterns are likely to be recognized by a machine learning model to predict sequence type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #import matplotlib pyplot\n",
    "\n",
    "base_counts = features.replace(['D', 'N', 'R', 'S'], 'ambig').apply(pd.Series.value_counts).fillna(0) # categorize all ambigous positions into one\n",
    "positions = list(range(-30,0)) + list(range(1, 31)) # get range of positions \n",
    "plt.bar(positions, base_counts.loc['A'], color = 'blue', width=1) # a stacked bar\n",
    "plt.bar(positions, base_counts.loc['T'], color = 'gold', bottom = base_counts.loc['A'], width=1) # t stacked bar\n",
    "plt.bar(positions, base_counts.loc['C'], color = 'red', bottom= base_counts.loc['A'] + base_counts.loc['T'], width=1) # c stacked bar\n",
    "plt.bar(positions, base_counts.loc['G'], color = 'green',bottom=base_counts.loc['A'] + base_counts.loc['T'] + base_counts.loc['C'], width=1) # g stacked bar\n",
    "plt.bar(positions, base_counts.loc['ambig'], color = 'grey', bottom= base_counts.loc['A'] + base_counts.loc['T'] + base_counts.loc['C'] + base_counts.loc['G'], width=1) # ambig bar\n",
    "plt.xlabel('Position') # add x label\n",
    "plt.ylabel('Frequency') # add y label\n",
    "plt.legend(['A', 'C', 'T', 'G', 'Ambig.']) # add legend\n",
    "plt.close() # shows just the embedded figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked Bar](https://github.com/dawsonz17/DS300project/raw/master/stacked_bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2 - 3 Pie Charts of Nucleotide Base Content in each Type of Splice Junction\n",
    "\n",
    "This figure calculates the percentage of each base across the classes of the data (IE, EI, N). It displays all of this information in a pie chart for each class that shows the percentage of A, T, C, G, and Undetermined across all members of that class. The pie charts shows a slight increase in GC content for Extron/Intron splices which is promissing for our hypothesis 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percent_basepair(text, letter): # function to get percent of a basepair\n",
    "    freq = text.count(letter) # gets frequency of each\n",
    "    percent = float(freq) / float(len(text)) # gets percent\n",
    "    return percent # returns percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EI</th>\n",
       "      <th>IE</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.214388</td>\n",
       "      <td>0.199135</td>\n",
       "      <td>0.245753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.210687</td>\n",
       "      <td>0.255922</td>\n",
       "      <td>0.238750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.237688</td>\n",
       "      <td>0.296014</td>\n",
       "      <td>0.246664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.304012</td>\n",
       "      <td>0.216608</td>\n",
       "      <td>0.252330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undetermined</th>\n",
       "      <td>0.033225</td>\n",
       "      <td>0.032321</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    EI        IE         N\n",
       "A             0.214388  0.199135  0.245753\n",
       "T             0.210687  0.255922  0.238750\n",
       "C             0.237688  0.296014  0.246664\n",
       "G             0.304012  0.216608  0.252330\n",
       "Undetermined  0.033225  0.032321  0.016502"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all reads for each sample and group by class (IE, EI, N)\n",
    "concatenated_reads = pd.DataFrame({'sum': df.sum(axis=1), 'Class': df['Class']}).groupby('Class')\n",
    "\n",
    "percentage = {}\n",
    "for name, group in concatenated_reads: # Iterate trhough groups to find percentage of each BasePair\n",
    "    all_reads = group.sum()['sum'] # Concatenate all reads for each group\n",
    "    total_percent = 0 # initialize variable\n",
    "    class_dict = {} # initialize dictionary\n",
    "    for base in ['A', 'T', 'C', 'G']: # iterate through each basepair\n",
    "        percent = calc_percent_basepair(all_reads, base) # calculate percentage of basepair\n",
    "        class_dict.update({base: percent}) # update dictionary\n",
    "        total_percent += percent # update total percent\n",
    "    \n",
    "    class_dict.update({'Undetermined': 1 - total_percent}) # calculate undetermined percentage\n",
    "    percentage.update({name: class_dict}) # update percentage\n",
    "    \n",
    "\n",
    "final_df = pd.DataFrame.from_dict(percentage) # create dataframe from read percentages\n",
    "final_df # show dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create plots\n",
    "\n",
    "#Extron/Intron\n",
    "final_df.plot(kind='pie', y='EI', figsize=(5, 5), colors = ['blue', 'gold', 'red', 'green', 'grey'])\n",
    "plt.title('Exon/Intron') # add title\n",
    "plt.legend(loc=\"lower right\",\n",
    "           borderaxespad=0.1,\n",
    "           bbox_to_anchor=(1.3, 0.6)) # move legend off the plot\n",
    "plt.close() # shows just the embedded figure\n",
    "\n",
    "#Intron/Extrong\n",
    "final_df.plot.pie(y='IE', figsize=(5, 5), colors = ['blue', 'gold', 'red', 'green', 'grey']) # create plot\n",
    "plt.title('Intron/Exon') # add title\n",
    "plt.legend(loc=\"lower right\",\n",
    "           borderaxespad=0.1,\n",
    "           bbox_to_anchor=(1.3, 0.6)) # move legend off the plot\n",
    "plt.close() # shows just the embedded figure\n",
    "\n",
    "#Non-splice\n",
    "final_df.plot.pie(y='N', figsize=(5, 5), colors = ['blue', 'gold', 'red', 'green', 'grey']) # create plot\n",
    "plt.title('Non-splice') # add title\n",
    "plt.legend(loc=\"lower right\",\n",
    "           borderaxespad=0.1,\n",
    "           bbox_to_anchor=(1.3, 0.6)) # move legend off the plot\n",
    "plt.close() # shows just the embedded figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pie 1](https://github.com/dawsonz17/DS300project/raw/master/pie1.png)\n",
    "![Pie 2](https://github.com/dawsonz17/DS300project/raw/master/pie2.png)\n",
    "![Pie 3](https://github.com/dawsonz17/DS300project/raw/master/pie3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3 - Distribution of Selected Position Features around Splice Junction\n",
    "\n",
    "Figure 3 displays which positions were selected from using univariate feature selection. It also shows which base of the given position it chose based off the colors in the bar for that position. This figure indicates what are likely the important positions for deciding class of splice junction. \n",
    "\n",
    "\n",
    "This plot provides a visual showcasing that the most important positions to classifying a splice junction are centered roughly around the middle of the sequence. It shows that the -11 to 8 position range are all relatively more important which is skewed slightly left of the middle of the read. It also hints that A and T are more important than C and G when deciding for a specific position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = df.drop('Class', axis=1).reset_index(drop=True) # isolate un-hot encoded features\n",
    "included = [[False] * 4] * len(features) # keep track of each attribute\n",
    "base_included = pd.DataFrame(columns =['A', 'T', 'C', 'G'])\n",
    "base_included\n",
    "for attribute in features.columns: # for each attribute\n",
    "    index = int(attribute.strip('attribute_')) - 1 # 'attribute_x' -> x - 1\n",
    "    selected_bases = [0,0,0,0]\n",
    "    all_bases = ['x%d_A'%index, 'x%d_T'%index, 'x%d_C'%index, 'x%d_G'%index]\n",
    "    for i, single_base in enumerate(all_bases): # check if any one of this index's bases was included in feature selection\n",
    "        if single_base in selected_feature_list:\n",
    "            selected_bases[i] = 1\n",
    "    base_included = base_included.append({'A' : selected_bases[0], 'T' : selected_bases[1], 'C' : selected_bases[2], 'G' : selected_bases[3]}, ignore_index = True) \n",
    "            \n",
    "\n",
    "colors = ['blue', 'gold', 'red', 'green'] # color list\n",
    "positions = list(range(-26,0)) + list(range(0, 30)) # isolate positions\n",
    "plt.figure(figsize=(10,7)) # create fig size\n",
    "for read_position, graph_position in zip(range(60), positions): # dataframe and graph have different indices (-30,30) instead of 0(0-60)\n",
    "    height = 0\n",
    "    for i, base in enumerate(['A', 'T', 'C', 'G']): # for each base\n",
    "        if base_included.iloc[read_position, i]: # if base is included in the selected features\n",
    "            plt.bar(graph_position, 1, color = colors[i], bottom = height, width=1, label=base) # add one one height bar per base used with corresponding color\n",
    "            height += 1\n",
    "    plt.bar(-30, 1, color = 'white', bottom = height, width=1, label=base) # expand to fix range with invisible bar\n",
    "    plt.bar(30, 1, color = 'white', bottom = height, width=1, label=base) # expand to fix range with invisible bar\n",
    "plt.xlabel('Sequence Position') # add x label\n",
    "plt.ylabel('Frequency') # add y label\n",
    "blue_patch = mpatches.Patch(color='blue', label='A') # add blue patch\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='T') # add yellow batch\n",
    "red_patch = mpatches.Patch(color='red', label='C') # add red patch\n",
    "green_patch = mpatches.Patch(color='green', label='G') # add green patch\n",
    "plt.legend(handles=[blue_patch, yellow_patch, red_patch, green_patch], loc=\"upper right\") # create custome legend\n",
    "plt.close() # shows just the embedded figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked Bar](https://github.com/dawsonz17/DS300project/raw/master/binary_bar_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Testing of the first and third hypothesis are found in other sections, organized by the most optimal way to test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing for Hypothesis 2\n",
    "\n",
    "First we check the assumptions required to run an ANOVA. We use a Shapiro test to test the normality of each category, as well as a Levene test to test for homogeneity of variances amongst the categories. All tests are statistically significant, so the assumptions are not met to run a standard ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeveneResult(statistic=9.704619548316602, pvalue=6.28230952133715e-05)\n",
      "ShapiroResult(statistic=0.9692525863647461, pvalue=1.2738653014376933e-11)\n",
      "ShapiroResult(statistic=0.9704548716545105, pvalue=2.413696499614648e-11)\n",
      "ShapiroResult(statistic=0.9960957765579224, pvalue=0.0002952643553726375)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats # import stats\n",
    "\n",
    "target_gc = pd.concat((target, counts['gccontent']), axis=1) # concat gc content\n",
    "gc_ei = target_gc[target_gc['Class'] == 0]['gccontent'] # isolate exon/intron gc content\n",
    "gc_ie = target_gc[target_gc['Class'] == 1]['gccontent'] # isolate intron/exon gc content\n",
    "gc_n = target_gc[target_gc['Class'] == 2]['gccontent'] # isolate non-splice gc content\n",
    "\n",
    "# Check assumptions of normality and homogeneity of variances\n",
    "levene_results = stats.levene(gc_ei, gc_ie, gc_n) # run levene\n",
    "shapiro_ei = stats.shapiro(gc_ei) # shapiro for exon/intron\n",
    "shapiro_ie = stats.shapiro(gc_ie) # shapiro for intron/exon\n",
    "shapiro_n = stats.shapiro(gc_n) # shapiro for non-splice\n",
    "print(levene_results) # print levene results\n",
    "print(shapiro_ei) # print shapiro for exon/intron\n",
    "print(shapiro_ie) # print shapiro for intron/exon \n",
    "print(shapiro_n) # print shapiro for non-splice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the assumptions are not met to run a standard ANOVA, we instead use a non-parametric Kruskal-Wallis test. The Kruskal-Wallis test returns a significant result, so we follow it with a non-parametric Dunn’s test for post hoc comparisons. The post-hoc comparisons real each category is significantly different from the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=112.01838914715542, pvalue=4.737136083325475e-25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EI</th>\n",
       "      <th>IE</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EI</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.005014e-07</td>\n",
       "      <td>9.625740e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE</th>\n",
       "      <td>6.005014e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.176927e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>9.625740e-26</td>\n",
       "      <td>3.176927e-06</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EI            IE             N\n",
       "EI  1.000000e+00  6.005014e-07  9.625740e-26\n",
       "IE  6.005014e-07  1.000000e+00  3.176927e-06\n",
       "N   9.625740e-26  3.176927e-06  1.000000e+00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All Shapiro tests and the Levene test were statistically significant\n",
    "# Assumptions of normality and equality of variances NOT met\n",
    "# Use Kruskal-Wallis non-parametric ANOVA instead\n",
    "from scikit_posthocs import posthoc_dunn # import post hoc tests\n",
    "\n",
    "kruskal_results = stats.kruskal(gc_ei, gc_ie, gc_n) # run Kruskal-Wallis test\n",
    "print(kruskal_results) # print results\n",
    "# Statistically significant\n",
    "# Use Dunn non-parametric post-hoc tests\n",
    "dunn_result = posthoc_dunn(target_gc, val_col='gccontent', group_col='Class') # pot hoc test\n",
    "dunn_result = dunn_result.rename({0:'EI', 1: 'IE', 2: 'N'}) # reformat results\n",
    "dunn_result = dunn_result.rename({0:'EI', 1: 'IE', 2: 'N'}, axis=1) # reformat results on other axis\n",
    "dunn_result # show results of post hoc tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the GC-content is significantly different between the categories, we calculate the median GC-content of each sequence type to figure out which has the highest and lowest GC-content. We use the median instead of the mean since non-parametric tests compare group medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exon/Intron Median GC content:  0.5833333333333334\n",
      "Intron/Exon Median GC content:  0.55\n",
      "Non-Splice Median GC content:  0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "# All categories are different from each other\n",
    "# Finding the median of each class to compare\n",
    "import numpy as np # import numpy\n",
    "\n",
    "ei_med = np.median(gc_ei) # calculate median of exon/intron\n",
    "ie_med = np.median(gc_ie) # calculate median of intron/exon\n",
    "n_med = np.median(gc_n) # calculate median of non-splice\n",
    "print(\"Exon/Intron Median GC content: \", ei_med) # print median gc content for exon/intron\n",
    "print(\"Intron/Exon Median GC content: \", ie_med) # print median gc content for intron/exon\n",
    "print(\"Non-Splice Median GC content: \", n_med) # print median gc content for non-splice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction and Initial Accuracies\n",
    "\n",
    "The following code block creates a list of all the machine learning algorithms we test and iteratively runs cross validation on all of them. The output for this cell is the mean accuracy and standard deviation of each model. The cross-fold validation uses k=10 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor:\n",
      "\tMean Accuracy: 84.24%\n",
      "\tStandard Deviation: 1.93%\n",
      "Support Vector Machine:\n",
      "\tMean Accuracy: 96.15%\n",
      "\tStandard Deviation: 0.59%\n",
      "Gaussian Naive Bayes:\n",
      "\tMean Accuracy: 92.64%\n",
      "\tStandard Deviation: 1.70%\n",
      "Decision Tree:\n",
      "\tMean Accuracy: 93.19%\n",
      "\tStandard Deviation: 1.46%\n",
      "Bernoulli Naive Bayes:\n",
      "\tMean Accuracy: 96.20%\n",
      "\tStandard Deviation: 0.84%\n",
      "Multinomial Naive Bayes:\n",
      "\tMean Accuracy: 96.07%\n",
      "\tStandard Deviation: 0.82%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # import k-nearest neighbors\n",
    "from sklearn.svm import LinearSVC # import svm\n",
    "from sklearn.naive_bayes import GaussianNB # import gaussian naive_bayes\n",
    "from sklearn.tree import DecisionTreeClassifier # import decision tree classifier\n",
    "from sklearn.naive_bayes import BernoulliNB # import Bernoulli NB\n",
    "from sklearn.naive_bayes import MultinomialNB # import multinomial naive bayes\n",
    "from sklearn.model_selection import KFold # import k-fold\n",
    "from sklearn.model_selection import cross_val_score # import cross validation scores\n",
    "\n",
    "# dictionary of machine learning algorithms (no hyperparameters tuned)\n",
    "estimatorsi={'K-Nearest Neighbor': KNeighborsClassifier(), # k-nearest neighbors\n",
    "            'Support Vector Machine': LinearSVC(max_iter=1000000), # svm\n",
    "            'Gaussian Naive Bayes': GaussianNB(), # gaussian naive_bayes\n",
    "            'Decision Tree': DecisionTreeClassifier(), #decision tree classifier\n",
    "            'Bernoulli Naive Bayes': BernoulliNB(), # bernoulli naive bayes\n",
    "            'Multinomial Naive Bayes': MultinomialNB()} # multinomial naive bayes\n",
    "\n",
    "def runkfold(estimators): # runs cross fold validation on a dictionary of models\n",
    "    for cmodel in estimators: # for each model     \n",
    "        kfold = KFold(n_splits=10, random_state=3000, shuffle=True) # setup KFold\n",
    "        scores = cross_val_score(estimator=estimators[cmodel], X=X_train_selected, y=y_train, cv=kfold) # calculate scores\n",
    "        print(cmodel + \":\") # print model name\n",
    "        print(f'\\tMean Accuracy: {scores.mean():.2%}') # print mean accuracy\n",
    "        print(f'\\tStandard Deviation: {scores.std():.2%}') # print standard deviation\n",
    "\n",
    "runkfold(estimatorsi) # find initial accuracies before hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Model Optimization\n",
    "\n",
    "Note: The order of the model optimization section and the model evaluation section was switched in order to create a cohesive report. Cells appear in the order they should, and were, run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search with Cross-Fold Validation\n",
    "\n",
    "The following code block runs grid search with cross validation on all 6 models in order to find the best hyperparameters for each model. This is done in order to ensure the most optimal model is used to minimize loss. Ensuring the best model is used reduces the risk of overfitting or underfitting a model and informs which hyperparameters should be used in the best model. In order to iterate through all the models, a dictionary of dictionaries is created to hold all the possible arguments for all relevant parameters for all models. The code block then outputs the best cross validation score obtained for each model through grid search with cross validation and also displays which hyperparameters were used to achieve that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " K-Nearest Neighbor\n",
      "Best Parameters:  {'algorithm': 'ball_tree', 'n_neighbors': 9}\n",
      "Best cross-validation score:  0.8465736672460931\n",
      "\n",
      " Support Vector Machine\n",
      "Best Parameters:  {'C': 10, 'max_iter': 10000}\n",
      "Best cross-validation score:  0.9598649557568505\n",
      "\n",
      " Gaussian Naive Bayes\n",
      "Best Parameters:  {'var_smoothing': 1e-07}\n",
      "Best cross-validation score:  0.9276770817864973\n",
      "\n",
      " Decision Tree\n",
      "Best Parameters:  {'max_depth': 5}\n",
      "Best cross-validation score:  0.9372882836453209\n",
      "\n",
      " Bernoulli Naive Bayes\n",
      "Best Parameters:  {'alpha': 0.1}\n",
      "Best cross-validation score:  0.9632122360915785\n",
      "\n",
      " Multinomial Naive Bayes\n",
      "Best Parameters:  {'alpha': 0.1}\n",
      "Best cross-validation score:  0.9627929525423433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV # import grid search with cross validation\n",
    "\n",
    "# dictionary of all models and their respective dictionaries of possible hyperparameters to be tuned by grid search\n",
    "param_grid = {'K-Nearest Neighbor': {'n_neighbors': [1, 2, 3, 5, 9], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}, # k-nearest neighbor\n",
    "              'Support Vector Machine': {'C': [10, 1, 0.1, 0.01, 0.001, 0.0001], 'max_iter': [10000, 100000, 1000000]}, #SVM\n",
    "              'Gaussian Naive Bayes': {'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}, # gaussian naive bayes\n",
    "              'Decision Tree': {'max_depth': [1, 5, 10, 15, 20, 25, 30]}, # decision tree\n",
    "              'Bernoulli Naive Bayes': {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001]}, # bernoulli naive bayes\n",
    "              'Multinomial Naive Bayes': {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001]}} # multinomial naive bayes\n",
    "\n",
    "\n",
    "for model in estimatorsi: # for each model\n",
    "    grid_search = GridSearchCV(estimatorsi[model], param_grid[model], cv=5) # find the best hyper parameters\n",
    "    grid_search.fit(X=X_train_selected, y=y_train) # find best hyper parameters\n",
    "\n",
    "    print(\"\\n\", model) # print model name\n",
    "    print(\"Best Parameters: \", grid_search.best_params_) # print best parameters (for use in next step)\n",
    "    print(\"Best cross-validation score: \", grid_search.best_score_) # print best cross-val score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Test Results for Hypothesis 2\n",
    "\n",
    "In the previous seciton, a Kruskal-Wallis non-parametric ANOVA was conducted to compare the GC-content among splice donors (exon-intron junctions), splice acceptors (intron-exon junctions), and non-splice sequences.\n",
    "Results revealed a statistically significant difference among the three sequence types, H = 112.02, p < .0001. \n",
    "Post-hoc comparisons using the non-parametric Dunn test indicated that the median GC content of splice donors (58.33%) was higher than that of splice acceptors (55%). Both of these sequence types had higher median GC content than non-splice sequences (51.67%).\n",
    "\n",
    "Note: Results from hypothesis testing for hypotheses 1 and 3 are found in the discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Results with Best Hyperparameters\n",
    "\n",
    "Using the best parameters determined using grid search in the previous section, cross fold validation was run once again to determine the best model. All models were run with the hyperparameters found to be most optimal using grid search. The K-Nearest Neighbor, Gaussian Naive Bayes, and Decision Tree algorithms performed well however the Support Vector Machine, Binomial Naive Bayes, and Multinomial Bayes algorithms performed relatively stronger. The two metrics evaluated at this step are the mean accuracy and the standard deviation for each model. Since models were run using cross-fold validation, the mean accuracy represents the average percent of correctly classified sequences of all the models trained during cross-fold validation. The standard deviation represents the percent standard deviation of those same values. Based on the output accuracies and standard deviations, the best algorithm was found to be the Binomial Naive Bayes with an alpha parameter of 0.1. Since cross-validation was used, the possible error induced by the specific selection of a training/validation set is reduced as cross-fold validation uses multiple splits and self-validates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor:\n",
      "\tMean Accuracy: 85.41%\n",
      "\tStandard Deviation: 1.66%\n",
      "Support Vector Machine:\n",
      "\tMean Accuracy: 96.15%\n",
      "\tStandard Deviation: 0.59%\n",
      "Gaussian Naive Bayes:\n",
      "\tMean Accuracy: 93.27%\n",
      "\tStandard Deviation: 2.00%\n",
      "Decision Tree:\n",
      "\tMean Accuracy: 92.89%\n",
      "\tStandard Deviation: 1.59%\n",
      "Bernoulli Naive Bayes:\n",
      "\tMean Accuracy: 96.32%\n",
      "\tStandard Deviation: 0.83%\n",
      "Multinomial Naive Bayes:\n",
      "\tMean Accuracy: 96.20%\n",
      "\tStandard Deviation: 0.82%\n"
     ]
    }
   ],
   "source": [
    "# dictionary of machine learning algorithms with best hyperparameters determined using GridSearch\n",
    "estimatorsf={'K-Nearest Neighbor': KNeighborsClassifier(algorithm='auto', n_neighbors=9), # k-nearest neighbors\n",
    "            'Support Vector Machine': LinearSVC(C=1, max_iter=10000), # svm\n",
    "            'Gaussian Naive Bayes': GaussianNB(var_smoothing=1e-07), # gaussian naive_bayes\n",
    "            'Decision Tree': DecisionTreeClassifier(max_depth=10), #decision tree classifier\n",
    "            'Bernoulli Naive Bayes': BernoulliNB(alpha=0.1), # bernoulli naive bayes\n",
    "            'Multinomial Naive Bayes': MultinomialNB(alpha=0.1)} # multinomial naive bayes\n",
    "\n",
    "runkfold(estimatorsf) # find highest accuracies after hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Results\n",
    "\n",
    "Based on the output of the previous cell, the Bernoulli Naive Bayes, Multinomial Bayes, and Support Vector Machine algorithms performed the best as evidenced by their high mean accuracies, and relatively low standard deviations. Among them, the Binomial Naive Bayes performed the best. On the contrary, the K-Nearest Neighbor, Guassian Naive Bayes, and Decision Tree algorithms did not perform as well as evidenced by their lower mean accuracies (as low as 85.41% for K-Nearest Neighbor) and relatively higher standard deviations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Bernoulli Naive Bayes on Test Set with Best Hyperparameters\n",
    "\n",
    "This section focused on testing the best algorithm using the testing set. Based on the above accuracies and standard deviations, the best model was the Bernoulli Naive Bayes model with 10 fold cross-validation and an alpha parameter of 0.1. This is based on its accuracy of 96.32% and standard deviation of 0.83%. This model will now be further evaluated to ensure there was no overfitting of the model by analyzing the confusion matrix, f-1 score, precision, and recall along with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       199\n",
      "           1       0.95      0.98      0.96       197\n",
      "           2       0.99      0.96      0.97       402\n",
      "\n",
      "    accuracy                           0.96       798\n",
      "   macro avg       0.96      0.97      0.96       798\n",
      "weighted avg       0.97      0.96      0.97       798\n",
      "\n",
      "Un-normalized Confusion Matrix\n",
      "[[192   5   2]\n",
      " [  1 193   3]\n",
      " [ 11   6 385]]\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "[[0.96482412 0.02512563 0.01005025]\n",
      " [0.00507614 0.97969543 0.01522843]\n",
      " [0.02736318 0.01492537 0.95771144]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEWCAYAAADy2YssAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXgUVdbA4d9JAiQQwhbCjiwqizAgIAoKog4M+KEioyICCjOO4giMjgziggvquKDouI3jAo7IiCLgCiqoiALKprJEEQTCEiABCYugkuR8f1QldJok3aQ76U7lvDz90FV169atm8rJvbc2UVWMMcaLYiJdAGOMKS0W4IwxnmUBzhjjWRbgjDGeZQHOGONZFuCMMZ5lAS7KiMhCEbnW/T5ERD4Kc/7NRERFJC6c+QbYpojIVBHZJyLLQsinh4isD2fZIkVEmorIIRGJjXRZvKzCBTgR2SIiu0Wkms+8a0VkYQSLVShVna6qfSJdjjA4B+gNNFbVriXNRFU/V9VW4StW6XCPsd8Xl0ZVt6pqoqrmlFW5KqIKF+BcccDfQs3EbZlU1Do8EScBW1T150gXJBqUZeu5oquov5yTgLEiUrOwhSLSXUSWi8h+9//uPssWisgDIrIYOAy0cLt8fxWRDSJyUETuE5GWIrJURA6IyBsiUtldv5aIvCcimW6X7T0RaVxEOYaLyBfu93Fulybvc1REXnaX1RCRl0Rkp4jsEJH787o+IhIrIo+KyB4R2QT8X3EVIyJNRGS2W769IvK0Oz9GRO4UkTQRyRCRV0Skhrssr9t7jYhsdbd1h7vsz8CLQDe33Pf67pfPdlVETna/XygiqW5d7hCRse78XiKy3WedNu7PI0tE1onIxT7LXhaRZ0TkfTefr0SkZRH7nFf+ESKyzf25jBSRM0RktZv/0z7pW4rIJ2797BGR6XnHkohMA5oC77r7O84n/z+LyFbgE595cSJSW0S2i8hFbh6JIrJRRK4u7mdlgqCqFeoDbAF+D8wG7nfnXQssdL/XBvYBw3BaeoPd6Tru8oXAVuA0d3klQIF3gCR3/q/Ax0ALoAaQClzjrl8H+CNQFagOzATe8infQuBa9/tw4ItC9qEJkA5c6E6/BfwHqAakAMuA691lI4Hv3XVqA5+65Y0rJN9Y4FvgcTeveOAcd9mfgI3uPiW69TfNXdbMzfMFIAHo4NZBm8L2o7D9ctc/2f2+E+jhfq8FdHK/9wK2u98rueW5HagMnA8cBFq5y18GfgK6uj+n6cCMIo6JvPI/5+5zH+AXt15TgEZABnCum/5knC53FaAusAh4wv8YKyT/V9x6TfCZF+em6QPscrf3AvBmpH9XvPCJeAHKfIePBbh2wH73APUNcMOAZX7rLAWGu98XAhP9litwts/0SuBWn+nHfH8B/NbtCOzzmV5IMQHO/eXIzx+o5waTBJ80g4FP3e+fACN9lvWh6ADXDcgsYtnHwF99plsBR93gkffL2thn+TLgysL2o4j98g1wW4HrgSS/NL04FuB6uAEhxmf5a8A97veXgRd9ll0IfF/EzyCv/I185u0FBvlMzwJuKmL9AcDX/sdYIfm3KGRenM+8p4A1OH+86kT6d8ULn4raRUVV1wLvAeP9FjUE0vzmpeH8Fc+zrZAsd/t8P1LIdCKAiFQVkf+4Xb0DOH/9a0rwZ9NeAtar6sPu9Ek4rZmdblcqC6c1l+KzP77l9d83X02ANFXNLmSZf72k4QS3ej7zdvl8P4y7zyXwR5yAlCYin4lItyLKs01Vc/3K5PtzOtHyBPszTBGRGW73+QDwKpAcIG8o/Ljx9TzOH96pqro3iPxMABU2wLnuBv5CwV+KdJyg4aspsMNnOpRHsNyC0/o5U1WTgJ7ufAm0ooiMd9f9s8/sbTgtuGRVrel+klT1NHf5TpzAladpMZvYBjSVwgfB/eulKZBNwSAQrJ9xuugAiEh934WqulxVL8EJ0m8BbxRRniZS8CSP/8+ptDyIcwz8zv0ZDqXgz6+o46PI48b9A/cfnG7sDXnjkSY0FTrAqepG4HVgjM/sucCpInKVOwA8CGiL09oLh+o4rYEsEamNE2QDEpF+bjkHqOoRn33YCXwEPCYiSe7JgJYicq6b5A1gjIg0FpFaHN9i9bUMJyA+JCLVRCReRM52l70G3CwizUUkEfgn8HoRrb1AvgVOE5GOIhIP3OOzn5XFuf6vhqoeBQ4AhV1K8RVOoBwnIpVEpBdwETCjBOU5UdWBQzg/w0bAP/yW78YZqzwRt7v//wl4FHjlBFr1pggVOsC5JuIM/ALgdg3647S09gLjgP6quidM23sCZxxtD/Al8EGQ6w3CGS/8To6dSX3OXXY1zkB7Ks4JkTeBBu6yF4APcYLKKpyTA4VS55qsi3AG0bcC293tAkwBpuF0qTfjDMKPDrLs/tv5AafeFwAbgC/8kgwDtrjdv5E4LST/PH4DLgb64dTls8DVqvp9Scp0gu4FOuGM4b7P8XX6IHCnO2QwNlBmItIZ+DtO+XOAh3Fae8X9MTJBEHdw0xhjPMdacMYYz7IAZ4zxLAtwxhjPsgBnjPGsqLzpV+ISVKokRboYUatj6yaBExkTwNerVu5R1bqh5BGbdJJq9pGA6fRI5oeq2jeUbZVEdAa4KklUaTM40sWIWosWT450EaJebEzA66YrvKqVY4q7qyUomn2EKq2uCJjul2+eCeZOj7CLygBnjCkvBKL4iWEW4IwxJSdATPTecGEBzhgTGone4QALcMaYEFgX1RjjZdaCM8Z4kmAtOGOMV4m14IwxHmZnUY0x3mQnGYwxXiVYF9UY42HWgjPGeJN1UY0xXiVArJ1kMMZ4lY3BGWO8ybqoxhgvsxacMcazrAVnjPEksVu1jDFeZrdqGWO8yU4yGGO8zLqoxhhPsufBGWO8y7qoxhgvs5MMxhjPsjE4Y4wniXVRjTFeZi04Y4xXiQU4Y4wXOU8stwBnjPEiESTGAlzUuuCs1jx480BiY4Rp73zJE9M+LrC8RvUEnr5jMM0bJ/PLr0cZ/cBrfLdpFwBJiQk8efsg2rRogAKj73+N5Wu3lP1OhNnHS1O54/HZ5OTmMvTibvzt6t4Flqsqt0+exYKlqVStUpknJwyhQ+sm7Ni9jxvvnUbG3oPExAjDBnTn+kG9AHjkhblMe2cpdWomAnDHDf3p3f20st61UrFgaSq3PzaLnNxchl3SjZuu6VNguapy22OzmL9kHQnxlXnmrqF0aN0EgFH3TeejL9aSXKs6S2bcHonih6xCtuBEJAdY4zNrhqo+JCILgbGquqK0th2smBhh0tjLuHTMv0nPyOKTqX9n3udrWb9ld36aW67pzZoNOxg2fgqnnJTCpLGXMWD0swA8dPOlfPzl9wy//WUqxcWSEF85UrsSNjk5uYx/dCYzn7yRhik16TPiUfr2aEer5g3y0yxYmsqmbZksmzmBleu2MO6RN/hwyi3ExsZw75hL6dC6CYd+/oULhk+iV9dW+euOvLIXNw65IFK7VipycnIZ98hMZj/t1NcF10yib4/2tG7hU19LUvlxWwYrZt3FirVbuOXh11kwdSwAV/3fmfzl8p7ccM+0SO1CyKI5wJXm+d0jqtrR5/NQKW6rRDq3PYlN2/eQlr6Xo9k5zJ7/NRf2bF8gTavm9Vi04gcANqRl0LRBberWTqR61Sp0P70l0975EoCj2TkcOHSkzPch3FalptGscV2aNUqmcqU4BvTuxLxFawqk+WDRGgZd2BURoUu75uw/dIRde/ZTP7lGfssksVo8pzarx86M/ZHYjTKzcl0azRsn59fXwD6dj6uvuYvWcKVbX2e0b86Bg059AXTvdDK1kqpGouhhIyIBP5ESvRewlIEGdWuwI2Nf/nR6RhYN6tYokGbthnT69+oAQKe2TWlSvxYN69bkpEbJ7Nl3iGcmXMVn/x3Lv24fRFUPtOB2ZmbRKKVm/nTDlJrszNzvl2Y/Df3S7PJLszV9L2t+2EHndiflz3tp5uecO+Qhxtw/nawDh0tpD8rWzswsGtWrlT/t1FdWwTQZhaTxSuCXID8RUpoBLkFEvvH5DCrFbZVIYX9YFC0w/cQrC6hZPYFFr/yD6y7vweofdpCTk0tcbAwdWjVmyuzFnHvNoxw+8hs3XV3+u1+qx88TvyNUC0nkW5eHDv/KiNte4v6bBlK9WgIAwweew/JZd/HptHHUq1ODu56cE9ZyR0pQ9UXx9VWeCYFbb5FswZXmSYYjqtox2MQich1wHQCVq5dWmQpIz9hPo5SCf1l3ZR4okObg4V8Zdf9r+dPfzrmLtPS9JMRXJj1zPyvXpQHwziffeiLANUypyY6MYy2Q9Iws6tdNOi5Nul+aeslOy/dodg4jbnuJy/7Qhf7ndchPk1LnWB7DLunGkLHPl9YulKmGKTXZsbtgL6C+Xy+gYUqtgGnKs5iY8LSTRKQv8C8gFnjRf1hLRGoArwJNcWLXo6o6tdiyhaVkYaCqz6tqF1XtInEJZbLNVd9tpWWTZJo2qE2luFgG9j6deZ+vLZAmKTGBSnHOzcRXX3IWS77+kYOHfyXjp4Ps2L2Pk5umANDzjFNZv3n3cdsob05v05TN2zJJS9/Lb0ezeWv+Kvr2KDgu+Yce7Xl97jJUlRVrN5OUGE/95BqoKjc98D9ObVaPG646v8A6eWNOAHM/W11gEL4869S2KZu2ZZK2Yw+/Hc1m9kcrj6uvfj3aMcOtr+VrjtWXV4SjBSciscAzQD+gLTBYRNr6JbsRSFXVDkAv4DERKXZcqEJfJpKTk8u4R2cx618jiY2JYfp7X/H95l2MuLQ7AFPnLKFVs3r8++4h5OTksn7LLkY/MCN//XGPzeb5e4dSuVIcW3bs5cb7/xepXQmbuLhYHhx7GVf87Vlyc3MZ3P8sWrdowMuzvwCcrmbv7m1ZsGQdXS+bSEJ8ZZ68cwgAX327iTfmLadty4b0GvYwcOxykIlPv83aDTsQhCYNavPo+KgbsSiRuLhYHvnH5Vw25llycpUhF51Fm5YNmDrLqa8RfzyH3mefxvwlqXQeOJGE+Eo8PWFo/vrX3jmVxSs3sjfrEKf1n8D4v1zIsEu6RWp3Tlz4xti6AhtVdROAiMwALgFSfdIoUF2ciJkI/ARkF1u8wsZTwqGQy0Q+UNXxwVwmElOtnlZpM7hUyuUFmYsnR7oIUS82ii8+jRZVK8esVNUuoeQRl9xCa/b/Z8B0e/87OA3Y4zPreVXNH6cQkcuAvqp6rTs9DDhTVUf5pKkOvAO0BqoDg1T1/WLLdwL7ckJUtdCHRKlqr9LapjGmbOWdZAjCngDBtLBM/FtffwC+Ac4HWgLzReRzVT1w3JquqBmDM8aUTxIjAT9B2A408ZluDKT7pRkBzFbHRmAzTmuuSBbgjDElJ2G70Hc5cIqINHdPHFyJ0x31tRW4AEBE6gGtgE3FZVqhTzIYY0IXjuvcVDVbREYBH+JcJjJFVdeJyEh3+XPAfcDLIrIGp0t7q6ruKTJTLMAZY0IUrgt5VXUuMNdv3nM+39OBPv7rFccCnDGmxE7gJENEWIAzxoQmeuObBThjTAgkfLdqlQYLcMaYkFgX1RjjXdEb3yzAGWNCYy04Y4wnRfp5b4FYgDPGhMQCnDHGs+y1gcYYz7IWnDHGm8QCnDHGo4TofoGOBThjTAjsLKoxxsNi7CSDMcaTxLqoxhiPEqwFZ4zxMGvBGWM8y04yGGO8ycbgjDFeJYg98NIY413WgjPGeJaNwRljvMnG4IwxXuXcixq9Ec4CnDEmJFEc3yzAGWNCY3cyGGO8yZ4Hd+JOb92ExV8+EeliRK1aZ4yKdBGi3t6vnop0ESoEex6cMcbD7HlwxhgPi+L4ZgHOGBMCsZMMxhiPsuvgjDGeZgHOGONZURzfLMAZY0JjLThjjDdF+c320fukOmNM1HMeeBn4E1ReIn1FZL2IbBSR8UWk6SUi34jIOhH5LFCe1oIzxoQkJgxNOBGJBZ4BegPbgeUi8o6qpvqkqQk8C/RV1a0ikhKwbCGXzBhToYkE/gShK7BRVTep6m/ADOASvzRXAbNVdSuAqmYEytQCnDGmxMS92T7QB0gWkRU+n+v8smoEbPOZ3u7O83UqUEtEForIShG5OlD5rItqjAlJkENse1S1SzHLC8tF/abjgM7ABUACsFREvlTVH4rKtMgAJyJPFbKBY1tWHVNMYY0xFUSYbtXaDjTxmW4MpBeSZo+q/gz8LCKLgA7AiQc4YEUJC2qMqSAE50xqGCwHThGR5sAO4EqcMTdfbwNPi0gcUBk4E3i8uEyLDHCq+l/faRGp5kZOY4zJF44GnKpmi8go4EMgFpiiqutEZKS7/DlV/U5EPgBWA7nAi6q6trh8A47BiUg34CUgEWgqIh2A61X1r6HtkjGm3JPwPQ9OVecCc/3mPec3PQmYFGyewZxFfQL4A7DX3cC3QM9gN2CM8bYwXSZSKoI6i6qq2/yidE7pFMcYU54I4bnQt7QEE+C2iUh3QEWkMjAG+K50i2WMKS+i+YGXwXRRRwI34lx0twPo6E4bYyq4YLqnUd1FVdU9wJAyKIsxphyK5i5qwBaciLQQkXdFJFNEMkTkbRFpURaFM8ZEPwniEynBdFH/B7wBNAAaAjOB10qzUMaY8iPIe1EjIpgAJ6o6TVWz3c+rFHMLlzGm4nDOogb+REpx96LWdr9+6j58bgZOYBsEvF8GZTPGRDsJ/oGWkVDcSYaVOAEtr/TX+yxT4L7SKpQxpvwol+9kUNXmZVkQY0z5k9dFjVZB3ckgIu2AtkB83jxVfaW0CmWMKT/KZQsuj4jcDfTCCXBzgX7AF4AFOGNMRC8DCSSYs6iX4TxBc5eqjsB5wFyVUi2VMaZcEIHYGAn4iZRguqhHVDVXRLJFJAnIAMrdhb4LlqRy22NvkpOby7BLunPz8D4Flqsq4x97k/mL15EQX5ln7x5Gh9ZNil33oeff55W3llCnZiIAE268mD5nn8ZPWYe4ZvxLfJ2axuD+ZzFp3BVlu7NhdEG3Njx4y2XExsQw7e0lPPHf+QWW16iewNMThtK8cTK//HaU0fdN57sfdwJww+DzGDagO6iSujGdGye+yq+/ZUdiN8Lu46Wp3DZ5Frm5uQy9uBs3XXP88XTb5FksWOIcT09PGJp/PI2+bzofLV5Lcq3qLH7t9vx1Hn5hLq+8vYRk93i684aL6H32aWW3UyVUrruowAr3dV0v4JxZPQQsC7SSiBxS1UQRaYZzc/56n8WTy3IMLycnl3888gZznh5Fw3o1Of+aSfTr2Z7WLRrkp5m/JJUft2aycvbdrFi7hVsemsGCl/8RcN0bBp/H6GG/L7C9KlUqcfvI/nz3Y3r+L3t5FBMjTBp3BZeOepr03Vl88t9/MG/RGtZv3pWf5pYRf2DND9sZNu4FTjmpHpNuvYIBf32KBnVrcP2gczlr0AP88utRpvzzTwzs05nX3vsqgnsUHjk5uYybNJNZT91Iw5Sa/H74JPr2KHg8LViSyqZtGSx/8y5WrN3C2EdeZ/6UsQAM7n8m117ek7/eO+24vG+48jxGDb2gzPYlHKI4vgXuoqrqX1U1y33wXG/gGrereiJ+VNWOPp8yHb9buW4LLZok06xxMpUrxTGwdyfmfra6QJq5n63myv/riohwRvvm7D94hF179ge1rr9qCVXo1rEl8ZUrleZulbrOpzVj07Y9pO3Yy9HsHGbPX8WF5/6uQJpWzeuzaLnzt2tD2m6aNqhN3drVAYiLiyW+SiViY2OoGl+ZXZn7y3wfSsOq1DSaN06mWSPnmLi0d2fmLVpTIM28RWsY1O/44wmg++knUyupaiSKHnaCECOBP5FSZIATkU7+H6A2EOd+Lzd2Zu6nUb1a+dMN69Vip98v287MrIJpUmqyMyMr4LovzFzE2YP/yaiJr5J14HAp7kXZa1C3Bjt278ufTt+9jwZ1axRIs3bDDvqf1xGATm1Pokn92k7dZe7nqVc/Zs279/H9vAc48PMRPv3q+zItf2nZmVHIsZKZVTBNYcdTEAH+xTcX0WPIg4y+b3r5OJ6i/GkixbXgHivm8+gJbqeliHzj8+nhn0BErst7Z2LmnswTzL54qsffWeZf6YUkQUSKXfdPf+zB13Pu4fPp46mXnMSdT8wOR3GjRmFjK/7V8cR/51MzqSqLpo/nukHnsvqH7eTk5FKjegIX9mxPx0vupk2/O6gaX5kr+p1RRiUvXYXdp+hfV4UeNwHyHTHwHFbOupvPpt1KveQkJvxrTskLWYai+V7U4i70PS+M2/lRVTsWl0BVnweeB+jcuUtY73VtmFLzuJZI/eQaxafJyKJ+3Rr8djS7yHVT6iTlz79mwNkMurnA4+PLvXT/lkq9WvndrDwHf/6FURNfzZ/+9u17SUvfy/lntSEtfS97sw4B8O6n39L1d815Y97ysil8KSr0WDnueKpV6PFUHN/j6epLujP4lv+EqcSlR4DYKB6EqxBvtu/U9iR+3JpJ2o49/HY0m9nzV9GvZ8GxpH492zPj/WWoKsvXbCYpMYH6yTWKXdf3l/29hd/SpmUDvGRVahotm9alacM6VIqLZWDvTsxbVHD8MSkxgUpxsQBcPaA7S77eyMGff2H7rp/o0r45CVWccchzz2jF+s27y3wfSsPpbZqyaVsmaenOMTFn/kr69WxfIE3fHu14fZ7v8RR/XBD053s8vf/Zt7RpUT6Op3J5s72XxMXF8si4K/jjmGfIyVGGXHwWbVo2YMqszwGnq9nn7NOYv3gdnS69l4T4Sjxz19Bi1wW4+8m3WPPDdkSEpg1q8/jtg/O3+buL7+Lgz79w9Gg2cz9bzaynbixwlq08yMnJZdwjbzDryRuJjRWmv/Ml32/axYiB5wAwdfYXtGpen3/fM4yc3FzWb97F6PumA7ByXRrvfPw1C1+9lZycXFav385/5yyO5O6ETVxcLA+PvZzLxzxLTq5y1UVn0bpFA6bO/gJwupq9zz6N+UtS6fLHiSTEV+KpCUPz1//LnVNZvGoje7MO0a7/BMZfdyFDL+7GPU+9zdoNx46nx8ZfGaldPCHRfKuWFDZWEJaMi79MZIqqPlnUup07d9HFX9l7p4tS64xRkS5C1Nv71VORLkLUq1YlZqWqdgklj/qntNMhk2cFTDf54tYhb6skgrlVS3AeWd5CVSeKSFOgvqoWey2cqia6/28BEsJQVmNMFIrmFlwwY3DPAt2AvP7XQeCZUiuRMaZciebLRIIZgztTVTuJyNcAqrrPfX2gMaaCEyAuis+iBhPgjopILO7lPyJSF8gt1VIZY8qNKI5vQQW4J4E5QIqIPIDzdJE7S7VUxphyQSJ8K1YgwbwXdbqIrMR5ZJIAA1TV3mxvjAHKeQvOPWt6GHjXd56qbi3NghljyodoPosaTBf1fY69fCYeaI5zTVv0P6jKGFOqBCL6QMtAgumiFrgHxX2SyPVFJDfGVCQRvhUrkBO+VUtVV4mINx4LYYwJmUTxWxmCGYP7u89kDNAJCO/zjIwx5ZIXXhtY3ed7Ns6YXOCbz4wxFUK5DXDuBb6JqvqPMiqPMaacieaXzhT3yPI4Vc3B6ZIaY8xxnNcGBv4El5f0FZH1IrJRRMYXk+4MEckRkcsC5VlcC24ZTnD7RkTeAWYCP+ctVFVvPZ/bGFMi4biTwe0tPoPzYqvtwHIReUdVUwtJ9zDwYTD5BjMGVxvYC5zPsevhFLAAZ0wFF8aTDF2Bjaq6CUBEZgCXAKl+6UbjnAMI6kqO4gJcinsGdS3HAlue0nlKpjGm3AmyAZcsIr5PsX3efQ9LnkbANp/p7cCZBbcjjYBLcRpbIQe4WCCRwl8GZAHOGAMIMcFdB7cnwBN9g4kzTwC3qmpOsCc2igtwO1V1YlC5GGMqJCFsN9tvB5r4TDcG0v3SdAFmuMEtGbhQRLJV9a2iMi0uwEXvuV9jTHQQiAvPINxy4BQRaQ7sAK4ErvJNoKrN8zcr8jLwXnHBDYoPcBeUuKjGmAohXC04Vc0WkVE4Z0djcV5MtU5ERrrLS/TS4eJe/PxTiUpqjKlQwvXAS1WdC8z1m1doYFPV4cHkWSHei2qMKT1RfCODBThjTMkJwb2aL1IswBljSk7C10UtDRbgjDEl5tzJYAHOGONR0RveLMAZY0IUxQ04C3DGmFBIVD8PzgKcMabE7CyqMcbT7CTDCVIgOyc30sWIWnu/eirSRYh6dc4ZG+kiVAwS3Y8sj8oAZ4wpH6yLaozxNGvBGWM8K3rDmwU4Y0wIBIi1FpwxxquiOL5ZgDPGhEKQKO6kWoAzxoTEWnDGGE9yLhOJ3ghnAc4YU3JiLThjjIfZrVrGGE9yHngZ6VIUzQKcMSYkdhbVGONZUdxDtQBnjAmNteCMMZ5kY3DGGO8SsbOoxhjvit7wZgHOGBMCey+qMcbToje8WYAzxoQqiiOcBThjTEisi2qM8azoDW8W4IwxoYriCGcBzhhTYoLdyWCM8aoofx5cNL+z1RhTDkgQn6DyEekrIutFZKOIjC9k+RARWe1+lohIh0B5WgvOGBMCCcuLn0UkFngG6A1sB5aLyDuqmuqTbDNwrqruE5F+wPPAmcXlay04Y0xIRAJ/gtAV2Kiqm1T1N2AGcIlvAlVdoqr73MkvgcaBMrUAZ4wpsWC6p258SxaRFT6f6/yyagRs85ne7s4ryp+BeYHKZ11UY0xogmuh7VHVLieYixaaUOQ8nAB3TqCNWoAzxoQkTJeJbAea+Ew3BtKP25bI74AXgX6qujdQphUywH28NJU7Hp9NTm4uQy/uxt+u7l1guapy++RZLFiaStUqlXlywhA6tG7Cjt37uPHeaWTsPUhMjDBsQHeuH9QLgGvvmMrGrRkAHDh4hKTqCSycdmtZ71pYfLw0ldsmzyLXrZ+brulTYLmqctvkWSxYso6E+Mo8PWEoHVo7x+bo+6bz0eK1JNeqzuLXbs9f5+EX5vLK20tIrpkIwJ03XETvs08ru50qRRec2YoHbxpAbGwM0979iiemfVJgeY3qCTx9+yCaN6rDL79lM/qfr/Pdpl0AJCXG8+RtV9CmRQNUldH/fJ3la9MisRslFqbLRJYDp4hIc2AHcCVwVcHtSFNgNpUo2moAAAvXSURBVDBMVX8IJtNSD3AiosBkVb3FnR4LJKrqPaW97cLk5OQy/tGZzHzyRhqm1KTPiEfp26MdrZo3yE+zYGkqm7ZlsmzmBFau28K4R97gwym3EBsbw71jLqVD6yYc+vkXLhg+iV5dW9GqeQNefGBE/vp3/WsOSYnxkdi9kOXk5DJu0kxmPeXUz++HT6Jvj/a0buFTP0tS2bQtg+Vv3sWKtVsY+8jrzJ8yFoDB/c/k2st78td7px2X9w1XnseooReU2b6UhZgYYdLYgVz6t/+QnrGfT166iXmfr2P9lt35aW65+gLWbEhn2G0vc8pJKUy6ZSADxjwHwEM3DeDjL9cz/I5XqBQXS0J8pUjtSsmE6To4Vc0WkVHAh0AsMEVV14nISHf5c8BdQB3gWffMbXaAbm+ZnGT4FRgoIsllsK2AVqWm0axxXZo1SqZypTgG9O7EvEVrCqT5YNEaBl3YFRGhS7vm7D90hF179lM/uUZ+SyWxWjynNqvHzoz9BdZVVd7++Gsu7d25zPYpnFalptG8cXJ+/Vzau/Nx9TNv0RoG9XPq54z2zdl/0KkfgO6nn0ytpKqRKHpEdG7blE3b95KW/hNHs3OYveBrLuxRsGXaqnk9Fq3YAMCGtAyaNqhF3VqJVK9ahe4dWzDt3a8AOJqdw4FDv5T5PoRKgvgXDFWdq6qnqmpLVX3AnfecG9xQ1WtVtZaqdnQ/xQY3KJsAl41zvcrNZbCtgHZmZtEopWb+dMOUmuzM3O+XZj8N/dLs8kuzNX0va37YQed2JxWYv/SbH6lbuzotm6aUQulL386MLBrVq5U/7dRPVsE0mYWlKVg/hXnxzUX0GPIgo++bTtaBw+ErdAQ1qFuDHbuP1U965n4a1K1RIM3aDen079UegE5tmtCkXi0aptTkpEZ12JP1M8/ccSWfvfx3/jX+CqrGVy7T8odKCNtlIqWirC4TeQYYIiI1AqYsZVrIeRn/vzBaSCLfH9Khw78y4raXuP+mgVSvllAg3ZyPVjKwnLbeoPDTVv4XchZaPwHyHTHwHFbOupvPpt1KveQkJvxrTskLGUUKPfXnVz1PTPuEmtUTWPTy37nu8nNYvWEHOTk5xMXG0OHURkyZs4Rzh0/m8C+/ctOw88uk3OEUrjsZSkOZBDhVPQC8AowpKo2IXJd3jcyezMxSK0vDlJrsyPD5i5uRRf26ScelSfdLUy/Zic1Hs3MYcdtLXPaHLvQ/r+CdItnZOby/cDUDep9eauUvbQ1TarJj97786fSMLOon1/BLU+v4NHWL/9uVUieJ2NgYYmJiuPqS7qxKLV8D6UVJz9xPo3o+rf26NfK763kOHv6VUQ+8Ts/hkxk58TWSayaSlv4T6Rn7Sc/cz8rUrQC88+lqOrQq7tKvKBXFEa4sL/R9AufalWqFLVTV51W1i6p2Sa5bt9QKcXqbpmzelkla+l5+O5rNW/NX0bdH+wJp/tCjPa/PXYaqsmLtZpIS46mfXANV5aYH/sepzepxw1XH/6X9bPl6Tm6WQsOUWsctKy9Ob9OUTdsySUvfw29Hs5kzfyX9ehasn7492vH6PKd+lq85Vj/F8f2lf/+zb2njc9KiPFv13TZaNk6maYPaVIqLZeDvT2feF+sKpElKjKdSXCwAV198Jku+2cTBw7+S8dNBduzO4uSmzvHes8sprN+8+7htRLsY981axX0ipcwuE1HVn0TkDZwgN6WstusvLi6WB8dexhV/e5bc3FwG9z+L1i0a8PLsLwAYPvAcendvy4Il6+h62UQS4ivz5J1DAPjq2028MW85bVs2pNewhwG444b+9O7uDCrPmb+qXHdPwamfh8dezuVjniUnV7nqIqd+prr1M2LgOfQ++zTmL0mlyx8nkhBfiacmDM1f/y93TmXxqo3szTpEu/4TGH/dhQy9uBv3PPU2azdsR0Ro2qA2j42/MlK7GFY5ObmMmzybWY9fR2ysMP29ZXy/eTcjBnQDYOpbS2nVrB7/njCYnFxl/eZdjH7wjfz1xz0+h+fvHkLlSrFsSf+JGx+YEaldKbEofpgIUth4Slg3IHJIVRPd7/Vwbph9pLjLRDp17qKLliwr1XKVZ9H8iOhoUeecsZEuQtT7ZfnklcGciSxOuw6ddPZHXwRM16p+tZC3VRKl3oLLC27u991AxbmGwBiPswdeGmO8K8ofeGkBzhgTkiiObxbgjDGhCM8DL0uLBThjTEiiOL5ZgDPGlFyk71QIxAKcMSY0URzhLMAZY0Jil4kYYzzLxuCMMd4kEGMBzhjjXdEb4SzAGWNKLO+Bl9HKApwxJiRRHN8swBljQmMtOGOMZ9mtWsYYz4re8GYBzhgTgki/NSsQC3DGmJDYnQzGGO+K3vhmAc4YE5oojm8W4IwxoYjsawEDsQBnjCmxaL+ToSxf/GyMMWXKWnDGmJBEcwvOApwxJiR2mYgxxpvsQl9jjFdF+0kGC3DGmJBYF9UY41nWgjPGeFYUxzcLcMaYEEVxhLMAZ4wpMYGovlVLVDXSZTiOiGQCaZEuh59kYE+kCxHFrH4Ci7Y6OklV64aSgYh8gLNfgexR1b6hbKskojLARSMRWaGqXSJdjmhl9ROY1VHZs3tRjTGeZQHOGONZFuCC93ykCxDlrH4CszoqYzYGZ4zxLGvBGWM8ywKcMcazLMAVQURyROQbn894d/5CEanwp/pF5JD7fzMROeJXV1dHunzRQkRURB7zmR4rIvdEsEgVit3JULQjqtox0oUoJ360uirSr8BAEXlQVaPpIt8KwVpwxpSubJyzpzdHuiAVkQW4oiX4dbsGRbpAUaylX131iHSBoswzwBARqRHpglQ01kUtmnVRg2dd1GKo6gEReQUYAxyJdHkqEmvBGVM2ngD+DFSLdEEqEgtwxpQBVf0JeAMnyJkyYgGuaP5jcA9FukBRzH8MbkykCxSlHiO4RwuZMLFbtYwxnmUtOGOMZ1mAM8Z4lgU4Y4xnWYAzxniWBThjjGdZgCunfJ52slZEZopI1RDyellELnO/vygibYtJ20tEupdgG1tE5LhLJIqa75fm0Alu6x4RGXuiZTTeYwGu/Dqiqh1VtR3wGzDSd6GIxJYkU1W9VlVTi0nSCzjhAGdMJFiA84bPgZPd1tWnIvI/YI2IxIrIJBFZLiKrReR6AHE8LSKpIvI+kJKXke/z7kSkr4isEpFvReRjEWmGE0hvzrupXkTqisgsdxvLReRsd906IvKRiHwtIv8hiPefi8hbIrJSRNaJyHV+yx5zy/KxiNR157UUkQ/cdT4XkdbhqEzjHXazfTknInFAP+ADd1ZXoJ2qbnaDxH5VPUNEqgCLReQj4HSgFdAeqAekAlP88q0LvAD0dPOqrao/ichzwCFVfdRN9z/gcVX9QkSaAh8CbYC7gS9UdaKI/B9QIGAV4U/uNhKA5SIyS1X34ty/uUpVbxGRu9y8R+E8hmikqm4QkTOBZ4HzS1CNxqMswJVfCSLyjfv9c+AlnK7jMlXd7M7vA/wub3wNqAGcAvQEXlPVHCBdRD4pJP+zgEV5ebn3Uhbm90BbkfwGWpKIVHe3MdBd930R2RfEPo0RkUvd703csu4FcoHX3fmvArNFJNHd35k+264SxDZMBWIBrvw67nFO7i/6z76zgNGq+qFfuguBQPfoSRBpwBnm6KaqBR4D5JYl6PsARaQXTrDspqqHRWQhEF9EcnW3m2WPaTLFsTE4b/sQuEFEKgGIyKkiUg1YBFzpjtE1AM4rZN2lwLki0txdt7Y7/yBQ3SfdRzjdRdx0eQFnETDEndcPqBWgrDWAfW5wa43TgswTA+S1Qq/C6foeADaLyOXuNkREOgTYhqlgLMB524s442urRGQt8B+cVvscYAOwBvg38Jn/iqqaiTNuNltEvuVYF/Fd4FKfJ/eOAbq4JzFSOXY2916gp4iswukqbw1Q1g+AOBFZDdwHfOmz7GfgNBFZiTPGNtGdPwT4s1u+dcAlQdSJqUDsaSLGGM+yFpwxxrMswBljPMsCnDHGsyzAGWM8ywKcMcazLMAZYzzLApwxxrP+H8rjDVsL4s54AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bmodel = BernoulliNB(alpha=0.1).fit(X=X_train_selected, y=y_train) # fit model to training with best hyperparameters\n",
    "\n",
    "from sklearn.metrics import confusion_matrix # import confusion matrix\n",
    "from sklearn.metrics import classification_report # import classification report\n",
    "from sklearn.metrics import plot_confusion_matrix # import plotting of confusion matrices\n",
    "\n",
    "predicted = bmodel.predict(X=X_test_selected) # creates predictions on testing set\n",
    "\n",
    "class_report = classification_report(y_true=y_test, y_pred=predicted) # creates classification report\n",
    "print(class_report) # reports precision, recall, f1-score and support as well as accuracies\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "disp = plot_confusion_matrix(bmodel, X_test_selected, y_test, \n",
    "                             display_labels=['EI', 'IE', 'N'], # labels\n",
    "                             cmap=plt.cm.Blues, # color\n",
    "                             normalize='true') # normalize plot to aid readability\n",
    "disp.ax_.set_title(\"Normalized confusion matrix\") # add axis title\n",
    "print(\"Un-normalized Confusion Matrix\") # organize results\n",
    "confusion = confusion_matrix(y_true=y_test, y_pred=predicted) # create confusion matrix (unnormalized)\n",
    "print(confusion) # print confusion matrix (unnormalized)\n",
    "print(\"\\nNormalized Confusion Matrix\") \n",
    "print(disp.confusion_matrix) # show confusion matrix\n",
    "plt.show() # show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Results\n",
    "\n",
    "The output of the previous cell provides more insight onto the performance of the Bernoulli Naive Bayes algorithm. The classification report showcases that the model evaluated on the test set had a high proportion of correctly predicted positive classes to all positive classes as evidenced by the high recall. The high precision score indicated a high proportion of correctly predicted positive classes out of all the positive predictions. In addition, a high F1 score with a weighted average for all three classes of 0.97 evidences a strong balance between precision and recall and a good performance by the model overall. As evidenced by the plot of the normalized confusion matrix, the model was very successful in minimizing false positives and false negatives as well as minimizing incorrect classifications without sacrificing too much accuracy.\n",
    "\n",
    "Overall, the results are very promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithms used were k-Nearest Neighbors, Support Vector Machine, Guassian Naive Bayes, Decision Tree, Binomial Naive Bayes, and Multinomial Naive Bayes. The support vector machine algorithm was chosen due to its strength as an algorithm to predict high dimensional data. The Multinomial Naive Bayes algorithm was chosen since it is optimized for discrete datasets such as ours. The Binomial Naive Bayes algorithm was chosen for a similar reason as the algorithm performs well on discrete data and more specifically binary data. While the majority of our features are not binary (A, C, T, or G), they were predicted to function similar to binary features once one-hot-encoding is used. Due to this reason, the Binomial Naive Bayes was included. The other used algorithms, including the K-Nearest Neighbors, Guassian Naive Bayes, and Decision Tree algorithms were not as suited for our dataset however were included as well due to the low computational cost of doing so. Finding the algorithm with the best performance was prioritized over the neglible computational cost of testing these additional algorithms even though the chance of their success was relatively lower than the other algorithms.\n",
    "\n",
    "Grid search with cross validation was used on all models with all relevant hyper parameters in order to ensure the most optimal model and hyperparameters were identified. The results of grid search are shown below:\n",
    "\n",
    " **K-Nearest Neighbor:**\n",
    "Best Parameters:  {'algorithm': 'auto', 'n_neighbors': 9},\n",
    "Best cross-validation score:  0.8511645746164576\n",
    "\n",
    " **Support Vector Machine:**\n",
    "Best Parameters:  {'C': 1, 'max_iter': 10000},\n",
    "Best cross-validation score:  0.9615394002789401\n",
    "\n",
    " **Gaussian Naive Bayes:**\n",
    "Best Parameters:  {'var_smoothing': 1e-07},\n",
    "Best cross-validation score:  0.93268479776848\n",
    "\n",
    " **Decision Tree:**\n",
    "Best Parameters:  {'max_depth': 10},\n",
    "Best cross-validation score:  0.9410547419804743\n",
    "\n",
    " **Bernoulli Naive Bayes:**\n",
    "Best Parameters:  {'alpha': 0.1},\n",
    "Best cross-validation score:  0.961952580195258\n",
    "\n",
    " **Multinomial Naive Bayes:**\n",
    "Best Parameters:  {'alpha': 0.1},\n",
    "Best cross-validation score:  0.9632043235704323\n",
    "\n",
    "Each algorithm was then run through cross fold validation with the optimal parameters found using grid search. The algorithms with the best general performances were the Support Vector Machine (C=1, max_iter= 10000) with a mean accuracy of 96.15%, the Multinomial Naive Bayes (alpha=0.1) with a mean accuracy of 96.20%, and the Binomial Naive Bayes (alpha=0.1) with a mean accuracy of 96.32%. These results match our hypothesis to the first question of the report. While all three of these algorithms had accuracies above 96%, the Binomial Naive Bayes performed the best and had a standard deviation of 0.83%. Due to this high performance, the Binomial Naive Bayes algorithm was tested using the testing set. The normalized confusion matrix shown in section 3.6 shows a very high accuracy and a low amount of false negatives / false positives indicating that there was minimized overfitting and underfitting.\n",
    "\n",
    "Based on these results, the Binomial Naive Bayes algorithm should be used for our predictive model. The features in our dataset can be used in order to predict the type of splice-junction in a given genetic sequence. The high accuracy of the trained model showcases that the nucleotide bases in a genetic sequence (especially those close to the center as shown by our third plot) as well as the GC content of a sequence provide valuable information that a supervised classification algorithm can use to predict the type of splice-junction in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Hypothesis Tests\n",
    "\n",
    "**1st Hypothesis Result** Our hypothesis that the Support Vector Machine, Multinomial Naive Bayes, and Binomial Naive Bayes algorithms would perform the best was supported by the final hyperparameter tuned model mean accuracies of 96.15%, 96.32%, and 96.2% for each respective model. The Binomial Naive Bayes algorithm performed the best with a weighted average F1 score of 0.97 on the testing set further answering the question of which machine learning model would perform the most optimally in classifying splice junctions in genetic sequences.\n",
    "\n",
    "\n",
    "**2nd Hypothesis Result** A non-parametric ANOVA with post-hoc comparisons revealed that all three sequence types had significantly different GC contents, with exon-intron sequences having the highest, followed by intron-exon sequences and lastly non-spice sequences. GC-content is a characteristic feature of a DNA sequence. The presences of differences in GC content indicates that GC content can serve as a diagnostic feature in predicting sequence type. Higher GC content also has structural implications for a DNA or RNA molecule, as G-C pairs are stronger than A-T pairs. This may indicate structure differences in splice sites compared to non-splice sites that the cell is able to recognize and interact with to correctly splice mRNA molecules.\n",
    "\n",
    "\n",
    "**3rd Hypothesis Result** The nucleotide bases in the middle of the sequence were selected at higher rates than those at the edges, aligning with our hypothesis. This finding is unsurprising though as the dataset was made to have the splice junctions in the middle, making them inherently important for identifying junctions. This can be seen in Figure 3. An interesting finding is that the bases leading up to the middle were more important than those following it. This can be seen with bases -12 to 8 being selected and very few selected out side this range. The lack of quantifiability to this hypothesis does not discredit the results as much as it proves the need for more analysis. It would be very interesting to do a more thorough analysis on codons as well as bases in the important regions to look for more patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion and Future Work\n",
    "\n",
    "The results of this analysis allowed the succesful answering of all our research questions. Out of the supervised classification algorithms used, the Binomial Naive Bayes algorithm performed the best with a mean accuracy of 96.32% in training and a weighted average F1 score of 0.97. Additionally, a significant difference was found in GC content between the types of splice junctions using a Kruskal Wallis test with p<0.0001. Another finding was that nucleotide bases at positions closer to the splice junction were found more important to the training of a supervised classification algorithm as evidenced by Figure 3.\n",
    "\n",
    "While the results of the models are promising, there are still some things that might produce even more interesting results. The first thing we could do is obtain more data to potentially increase the accuracy of the model. It would be especially interesting to see how this model can transfer to genes of non-primates. We also believe that more insights can be derived if an n-gram-like protocol is employed. Genes are transcribed into amino acids in groups of 3 bases called codons. Grouping the bases into n-grams of size 3 will allow for the recognition of specific amino acids that may be linked to splice junctions. This would be especially useful when identifying the beginning and end of genes. These insights, combined with the patterns discovered already with specific bases will allow for greater accuracy in predicting splice junctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "    Charles completed the problem statement, and significance of the problem. Questions, hypotheses and justifications for those hypotheses were worked on as a group.\n",
    "    \n",
    "**Data Aquisition** \n",
    "    This section was completed primarily as a group and Zach created the GitHub to load our dataset from.\n",
    "    \n",
    "**Data Analysis**\n",
    "    This section was completed by Giona.\n",
    "    \n",
    "**Data Wrangling**\n",
    "    This section was completed as a group.\n",
    "    \n",
    "**Data Exploration (plots)**\n",
    "    The first plot (stacked bar chart) was created by Charles. The rest of the plots were created by Zach. Zach also embedded the plotly images into the notebook from GitHub.\n",
    "    \n",
    "**Model Construction Evaluation and Testing**\n",
    "    This section was completed as a group.\n",
    "    \n",
    "**Discussion and Hypothesis Tests**\n",
    "    The discussion was completed as a group. Our first hypothesis and analysis of the findings from 3.4, 3.5, and 3.6 were evaluated by Giona, our second hypothesis was tested and evaluated by Charles, and our third hypothesis was evaluated by Zach. Zach also completed the future works section of the discussion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
